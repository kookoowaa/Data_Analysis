# 04. 단어 수준 임베딩 - NPLM, Word2Vec, FastText

## NPLM

> - NPLM<sup>Neural Probablistic Language Model</sup>은 자연어 처리 분야에 임베딩 개념을 퍼뜨리는 데 일조한 선구자적 모델로 알려져 있음
> - 2003년 요슈아 벤지오  연구팀이 제안한 기법으로, 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생

### 모델 기본 구조

- 전통적인 언어 모델이 갖고 있는 다음 3가지 한계 극복을 위함

  > 1) 학습데이터에 존재하지 않는 n-gram이 포함된 문장의 확률값을 0으로 부여하고
  >
  > 2) 장기 의존성(5-gram 이상)을 포착하기 어려울 뿐 아니라
  >
  > 3) 단어/문장 간유사도를 계산할 수 없음

### NPLM의 학습

- NPLM은 단어 시퀀스가 주어졌을 때 다음 간어가 무엇인지 맞추는 과정에서 학습을 진행

- $n-1$개 단어들로 다음 단어를 맞추는 n-gram 언어모델의 한 종류로, $P(w_t|w_{t-1},...,w_{t-n+1}) = \frac{exp(y_{wt})}{\sum_iexp(y_i)}$을 최대화 하며 학습을 진행

- 입력을 살펴 보면, **문장 내 $t$번째 단어($w_t$)에 대응하는 벡터 $C(w_i)$를 만드는 과정**임

- $C(w_i)$벡터는 $|V|$(어휘 집합 크기) x $m$(차원 수)로 구성되어 있으며, 행렬 $C$에서 **$w_i$를 참조**하는 형태로 이루어져 있음

- **참조**라 함은 원핫 벡터를 **내적**하였다고 볼 수 있으며, $C$라는 행렬에서 $w_t$에 해당하는 행만 참조하는 것과 동일함

- `[발, 없는, 말이, 천리, 간다]`에서 `[없는, 말이, 천리]`로 `간다`를 예측해야되는 상황이라고 가정, 프로세스는 다음과 같음:

  > 1. NLPM의 입력 벡터 x로 위의 모델을 설명하면, $t=5$, $n=4$(입력값 3 + 예측값1)을 갖게 되고 수식으로 표현하면 아래와 같이 표현할 수 있음
  >    $$x=[x_{t-1}, x_{t-2},...,x_{t-n+1}]=[x_4, x_3, x_2]$$
  > 2. 소프트맥스 함수를 출력층으로 스코어 벡터 $y_{wt}=b+Wx+Utanh(d+Hx)$를 계산하고, 이를 정답 단어인 `간다`의 인덱스와 비교하며 역전파 하는 방식으로 학습이 이루어짐
  > 3. 학습이 완료되면 행렬 $C$를 각 단어에 해당하는 m차원 임베딩으로 사용

### NPLM과 의미 정보

- Bengio et al. (2003)에 따르면 NPLM이 단어의 임베딩을 녹이는 과정을 다음 예제로 설명하고 있음

  ```
  The cat is walking in the bedroom
  A dog was running in a room
  The cat is running in a room
  A dog is walking in a bedroom
  The dog was walking in the room
  ```

-  위의 예제에서 `walking`이라는 단어를 $4-gram NPLM$ 모델로 학습하게 되면, `The`, `cat`, `is`, `A`, `dog`, `was`와 모종의 관계를 형성, 앞의 단어들에 해당하는 $C$ 행렬 벡터들은 `walking`을 맞추는 학습 손실을 최소화하는 **그래디언트<sup>gradient</sup>**를 받아 동일하게 업데이트를 진행함

-  따라서 `walking`이란 단어를 학습할 때 위의 단어들은 벡터 공간에서 같은 방향으로 움직이게 됨

-  동일한 방식으로 `room`을 학습하게 되면 이번에는 `running`, `in`, `a`, `walking`, `the`에 해당하는 $C$행렬 벡터들을 같은 방향으로 업데이트함

-  위와 같이 문장 내 모든 단어들을 한 단어씩 훑으면서 말뭉치 전체를 학습하면, NPLM 모델의 $C$행렬에 각 단어의 문맥 정보를 내재할 수 있게 됨

## Word2Vec

> - `Word2Vec`은 구글 연구팀이 2013년 발표한 기법으로, 가장 널리 쓰이고 있는 단어 임베딩 모델 중 하나임
> - `Word2Vec`은 Mikolov et al. (2013a) 에서 제안한 **Skip-Gram**과 **CBOW**라는 모델을 근간으로 Mikolov et al. (2013b)에서 네거티브 샘플링 등 학습 최적화 기법을 제안한 내용이 골자임

### 모델 기본 구조

-  `Word2Vec`에서 제안한 **Skip-Gram**과 **CBOW** 모델의 구조는 다음과 같음
-  

### 학습 데이터 구축

-  

### 모델 학습

-  

### 튜토리얼

-  

## FastText

### 모델 기본 구조

-  

### 튜토리얼

-  

### 한글 자소와 FastText

-  

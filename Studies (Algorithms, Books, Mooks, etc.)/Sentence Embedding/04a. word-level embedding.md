# 04. 단어 수준 임베딩 - NPLM, Word2Vec, FastText

## NPLM

> - NPLM<sup>Neural Probablistic Language Model</sup>은 자연어 처리 분야에 임베딩 개념을 퍼뜨리는 데 일조한 선구자적 모델로 알려져 있음
> - 2003년 요슈아 벤지오  연구팀이 제안한 기법으로, 통계 기반의 전통적인 언어 모델의 한계를 극복하는 과정에서 탄생

### 모델 기본 구조

- 전통적인 언어 모델이 갖고 있는 다음 3가지 한계 극복을 위함

  > 1) 학습데이터에 존재하지 않는 n-gram이 포함된 문장의 확률값을 0으로 부여하고
  >
  > 2) 장기 의존성(5-gram 이상)을 포착하기 어려울 뿐 아니라
  >
  > 3) 단어/문장 간유사도를 계산할 수 없음

### NPLM의 학습

-  NPLM은 단어 시퀀스가 주어졌을 때 다음 간어가 무엇인지 맞추는 과정에서 학습을 진행
-  $n-1$개 단어들로 다음 단어를 맞추는 n-gram 언어모델의 한 종류로, $P(w_t|w_{t-1},...,w_{t-n+1}) = \frac{exp(y_{wt})}{\sum_iexp(y_i)}$을 최대화 하며 학습을 진행
-  입력을 살펴 보면, **문장 내 $t$번째 단어($w_t$)에 대응하는 벡터 $C(w_i)$를 만드는 과정**임
-  $C(w_i)$벡터는 $|V|$(어휘 집합 크기) x $m$(차원 수)로 구성되어 있으며, 행렬 $C$에서 **$w_i$를 참조**하는 형태로 이루어져 있음
-  **참조**라 함은 원핫 벡터를 **내적**하였다고 볼 수 있으며, $C$라는 행렬에서 $w_t$에 해당하는 행만 참조하는 것과 동일함

### NPLM과 의미 정보

-  

## Word2Vec

### 모델 기본 구조

-  

### 학습 데이터 구축

-  

### 모델 학습

-  

### 튜토리얼

-  

## FastText

### 모델 기본 구조

-  

### 튜토리얼

-  

### 한글 자소와 FastText

-  

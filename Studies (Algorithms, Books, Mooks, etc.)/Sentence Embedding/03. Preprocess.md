# 03. 한국어 전처리

## 데이터 확보

- 임베딩용 데이터는 직접 만들수도 있고 웹에서 스크래핑하는 방법도 있지만, 여기에서는 공개되어 있는 말뭉치 데이터를 활용
- [00. Environment](00.%20Environment.md) 에서 만들어둔 docker 환경을 사용하며, 기본적으로 [git 환경](https://github.com/ratsgo/embedding.git)이 구축되어 있는 상태임

### 한국어 위키백과

- 한국어 위키백과의 원 데이터를 내려받는 코드는 `preprocess.sh`에 작성이 되어 있으므로 해당 파일을 `dump-raw-wiki` 명령어로 실행하고 전처리하여 원하는 형태의 데이터로 정리 

  ```bash
  git pull origin master
  bash preprocess.sh dump-raw-wiki
  bash preprocess.sh process-wiki
  ```

- 전처리 과정 중에 유용한 Regular expression은 아래와 같으며, `preprocess.sh`에 정의되어 있음:

  ```python
  import re
  
  WIKI_REMOVE_CHARS = re.compile("'+|(=+.{2,30}=+)|__TOC__|(ファイル:).+|:(en|de|it|fr|es|kr|zh|no|fi):|\n", re.UNICODE)
  
  WIKI_SPACE_CHARS = re.compile("(\\s|゙|゚|　)+", re.UNICODE)
  
  EMAIL_PATTERN = re.compile("(^[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+$)", re.UNICODE)
  
  URL_PATTERN = re.compile("(ftp|http|https)?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", re.UNICODE)
  
  WIKI_REMOVE_TOKEN_CHARS = re.compile("(\\*$|:$|^파일:.+|^;)", re.UNICODE)
  
  MULTIPLE_SPACES = re.compile(' +', re.UNICODE)
  ```

- 이 외 `wikiextractor` 또한 위키의 자료를 정리하는데 자주 사용되는 라이브러리로 다음의 ![링크](https://github.com/attardi/wikiextractor)에서 내용 확인 가능 

### KorQuAD
- [KorQuAD](https://korquad.gihub.io)는 한국어 기계독해를 위한 데이터 세트로 LG CNS가 구축하여 2018년에 공개 (70,079건)
- 질문과 답변 쌍을 사람들이 직접 만들어 둔 데이터로, 한국어 수능 지문과 유사한 형태의 구성을 갖추고 있음
- 마찬가지로 `preprocess.sh`에 데이터를 내려받고 정제하는 함수가 정의되어 있으므로 아래와 같이 실행 시 json 파일을 txt 파일로 정리:
```bash
git pull origin master
bash preprocess.sh dump-raw-korquad
bash preprocess.sh process-korquad
```

### 네이버 영화 리뷰 말뭉치
- [네이버의 영화 리뷰 말뭉치](https://github.com/e9t/nsmc)는 박은정님(lucypark)께서 구충하고 정제하여 공개한 말뭉치로, 감성분석 또는 문서분류에 제격인 데이터셋임
- 레코드 하나는 문서ID, 문서 내용, 1-0 레이블로 구성되어 있는 총 20만 세트의 tsv임
- 마찬가지로 `preprocess.sh`에 데이터를 내려받고 정제하는 함수가 정의되어 있으므로 아래와 같이 실행하여 전처리를 수행:
```bash
git pull origin master
bash preprocess.sh dump-raw-nsmc
bash preprocess.sh process-nsmc
```

### 전처리 완료 된 데이터 다운로드
- 위 내용을 참조하되, 아래와 같이 `preprocess.sh`를 실행하면, 전처리가 완료된 데이터셋을 한꺼번에 내려받을 수 있음
```bash
git pull origin master
bash preprocess.sh dump-processed
```
- 위 코드를 실행하면 모든 파일이 `/notebook/embedding/data/processed` 디렉터리에 저장되며, 파일 별 내용은 아래와 같음
  |파일명|내용|
  |---|---|
  |processed_wiki_ko.txt|한국어 위키백과|
  |processed_korquad.txt|KorQuAD 학습/데브세트|
  |processed_ratings.txt|네이버 영화 말뭉치 학습/테스트세트(극성레이블 없음)|
  |processed_ratings_train.txt|네이버 영화 말뭉치 학습세트(극성레이블 있음)|
  |processed_ratings_test.txt|네이버 영화 말뭉치 테스트세트(극성레이블 있음)|
  |space-correct.model|네이버 영화 말뭉치로 학습한 띄어쓰기 교정 모델|
  |corrected_ratings_train.txt|띄어쓰기 교정한 네이버 영화 말뭉치 학습세트(레이블 있음)|
  |corrected_ratings_test.txt|띄어쓰기 교정한 네이버 영화 말뭉치 테스트세트(레이블 있음)|
  |soyword.model|띄어쓰기 교정한 네이버 영화 말뭉치로 학습한 soynlp 형태소 분석 모델|
  |processed_review_movieid.txt|네이버 영화 말뭉치 전체 데이터세트(영화 ID 포함)|


##  지도 학습 기반 형태소 분석
> - 품질 좋은 임베딩을 만들기 위해서는 문장이나 단어의 경계를 컴퓨터에 잘 알려주어야 하며, 다행히 오픈소스로 다양한 한국어 형태소 분석기가 존재함
> - 아래에서 비지도 학습 기반의 형태소 분석기도 설명 할 예정이며, 우선 이 절에서는 지도학습 기반의 형태소 분석 방법을 설명
> - 지도학습 기반의 형태소 분석기는 언어학 전문가들이 태깅한 형태소 분석 말뭉치로부터 태깅된 정답 패턴에 최대한 가깝게 토크나이즈 함

### KoNLPy 사용법

> - `KoNLPy`는 한국어 자연어 처리 패키지로, 다양한 언어로 개발된 오픈소스 형태소 분석기를 (`은전한닢(Mecab)`, `꼬꼬마`, `한나눔`, `Okt`, `코모란`) 한군데에 모아 두었음
>
> - 기본적으로 동일한 방식으로 형태소를 분석하며, 처음 `tokenizer` 클래스를 선언할 때에만 다른 오픈소스를 불러오면 됨
>
> - 가장 보편적으로 사용되는 `은전한닢`을 사용한 결과는 아래와 같음:
>
>   ```python
>   from konlpy.tag import Mecab
>   tokenizer = Mecab() #tokenizer 선언
>   tokenizer.morphs("아버지가방에들어가신다")
>   	## ['아버지', '가', '방', '에', '들어가', '신다']
>   tokenizer.pos("아버지가방에들어가신다")
>       ## [('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EC')]
>       ## [('토큰', '품사')] (형태소와 품사) 쌍으로 태킹
>   ```
> ```
>   
> - `KoNLPy`의 형태소 분석기 중에서는 `은전한닢(Mecab)`이 일반적으로 성능 및 속도면에서 여타 형태소 분석기보다 좋은 성능을 내는 경우가 자주 있음
> ```

### Khaiii 사용법

> - `Khaiii`는 카카오가 2018년말에 공개한 오픈소스 한국어 형태소 분석기로, 국립국어원이 구축한 세종코퍼스를 이용해 CNN 모델로 학습한 모델임
>
> - 아키텍처의 개요는 입력문장을 문자 단위로 입력받은 후 컴볼루션 필터가 이 문자들을 슬라이딩해 가면서 정보를 추출
>
> - `Khaiii`는 `KoNLPy`와 마찬가지로 클래스 선언 이후에 활용이 가능하지만, `analyze()`라는 모듈함수로 형태소를 분석
>
> - 사용 방식은 다음과 같음:
>
>   ```python
>   from khaiii import KhaiiiApi
>   tokenizer = KhaiiiApi()
>   
>   data = tokenizer.analyze("아버지가방에들어가신다")
>   tokens = []
>   for word in data:
>       tokens.extend([str(m).split("/")[0] for m in word.morphs])
>   tokens
>   	## ['아버지', '가', '방', '에', '들어가', '시', 'ㄴ다']
>   tagging = []
>   for word in data:
>       tagging.extend([str(m) for m in word.morphs])
>   tagging
>   	## ['아버지/NNG', '가/JKS', '방/NNG', '에/JKB', '들어가/VV', '시/EP', 'ㄴ다/EC']
>   ```
>
> - `Khaiii`로 `analyze()` 모듈함수를 구동시키면, 의미단위의 `KhaiiiWord object`로 끊겨 있는 리스트(`data`)를 반환
>
> - 각각의 `KhaiiiWord object`는 다시 `morphs`함수를 통해 `형태소/품사` 형태의 `KhaiiiMorph object`들고 구성된 리스트 (`word.morphs`)를 반환함

### 은전한닢에 사용자 사전 추가하기

> - 때로는 중요한 토큰들이 형태소 분석기에 따라 n개의 무의미한 토큰으로 분류되어 기록되기도 함
>
> - 예를 들어 `교촌치킨`은 분석기에 따라 `['교촌치킨']`으로 분류되기도, `['교촌', '치킨']`으로 분류되기도 함:
>
>   ```python
>   from konlpy.tag import Mecab
>   tokenizer = Mecab()
>   tokenizer.morphs("교촌치킨 정말 좋네요")
>       ##['교촌', '치킨', '정말', '좋', '네요']
>   ```
>
> - 이때 `교촌치킨`이 중요한  관심 단어였다면, 하나의 토큰으로 인식되는 편이 임베딩 품질 측면에서 바람직함
>
> - `은전한닢`을 기준으로 사용자 사전을 추가할 수 있으며, `/notebooks/embedding/preprocessmecab-user-dic.csv/`에 단어를 추가할 수 있음
>
> - 사용자 사전에 추가할 때 포맷은 다음과 같음:
>
>   ```
>   교촌치킨,,,,,NNP,**,T,교촌치킨,**,**,**,**,**
>   ```

## 비지도 학습 기반 형태소 분석

- 

### soynlp 형태소 분석기

> - 

### 구글 센텐스피스

> - 

### 띄어쓰기 교정

> - 

### 형태소 분석 완료된 데이터 다운로드

> - 
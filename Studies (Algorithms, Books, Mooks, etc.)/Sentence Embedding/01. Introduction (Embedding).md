# 01. Embedding이란

- 자연어 처리에서 임베딩<sup>Embedding</sup>이란, 사람이 쓰는 자연어를 기계가 이해할 수 있는 **숫자의 나열**인 벡터<sup>Vector</sup> 공간에 "끼워 넣는다<sup>Embed</sup>"는 의미에서 유래됨
- 컴퓨터는 어디까지나 **계산기**일 뿐이기에, 자연어를 컴퓨터가 연산하고 처리할 수 있는 방식으로 치환해 주는 과정임

## 임베딩의 역할

### 단어/문장 간 관련도 계산

- 가장 단순한 형태의 임베딩은 **단어-문서 행렬<sup>Term-Document Matrix</sup>**로, 단어와 문서의 빈도 관계를 표현한 빈도표가 있음
- 현업에서 널리 알려진 임베딩 기법으로는 2013년 구글 연구팀이 발표한 **Word2Vec**이라는 기법이 있으며, 사람이 이해하기 어려운 고차원 (100개?) 벡터로 구성됨
- 단어를 벡터로 구성하게 되면, 벡터들 간에 유사도를 계산하는 것이 가능해지고, 보편적으로 **코사인 유사도<sup>cosine similarity</sup>**를 활용하여 유사도가 높은 벡터를 의미하는 단어를 역으로 추정하는 계산도 가능 (차원축소 및 시각화도 가능)

### 의미/문법 정보 함축

- 임베딩의 결과물인 벡터는 숫자로 이루어진 만큼, 사칙연산을 통해 단어 사이의 **의미/문법적 관계**를 도출하는 것도 가능
- 예를 들어 `아들 - 딸 + 소녀`연산의 결과인 벡터와 코사인 유사도가 가장 높은 단어가 `소년`과 같은 단어 유추평가가 성립된다면, 각 벡터 사이의 관계 및 의미 차이가 임베딩을 통해 벡터에 함축되어 있다고 볼 수 있음

### 전이 학습

- 임베딩 자체로도 위의 유추 평가 처럼 직접적으로 사용하기도 하지만, 임베딩 벡터를 입력값으로 하는 딥러닝 모델에서도 자주 사용됨; 이를 **전이학습<sup>transfer learning</sup>**이라고 함
- 전이 학습의 개념은, 마치 사람이 무언가를 배울 때 제로베이스에서 시작하지 않고 평생 쌓아온 지식을 바탕으로 새로운 사실을 빠르게 이해하듯이, 임베딩이라는 사전 지식을 활용하여 분류/예측/요약 등의 과제를 빠르고 잘 할 수 있게 도움을 줌

## 임베딩 기법의 역사

### 통계 기반에서 뉴럴 네트워크 기반으로

- 초기 임베딩 기법은 대부분 단어-문서 행렬 같은 통계량을 직접적으로 활용하는 기법에 의존적이었음
- 대표적인 기법이 **잠재 의미 분석<sup>Latent Semantic Analysis</sup>**로 거대한 **희소행렬<sup>sparse matrix</sup>**를 차원축소하여 단어 혹은 문서 수준의 임베딩을 구사하였음(**TF-IDF 행렬<sup>Term Frequency-Inverse Document Frequency</sup>, 단어-문맥 행렬<sup>Word-Context Matrix</sup>, 점별 상호 정보량 행렬<sup>Pointwise Mutual Information Matrix</sup>** 등 )
- 최근 임베딩 기법들은 뉴럴 네트워크 기반의 기법들이 주목받고 있음
- 뉴럴 네트워크는 그 구조가 유연하고 표현력이 풍부하기 때문에 자연어의 문맥을 상당 부분 효과적으로 학습할 수 있으며, 주로 **예측 과정에서 학습**을 진행하게 됨

### 단어 수준에서 문장 수준으로

- 2017년 이전의 임베딩 기법들은 **Word2Vec, GloVe**와 같은 단어 수준 모델이었으며, 단어의 벡터에 해당 단어의 문맥적 의미를 함축함 (동음이의어에 취약)
- 2018년 **ELMo<sup>Embeddings from Language Models</sup>** 발표 이후 단어 시퀀스 전체의 문맥적 의미를 함축하는 문장 수준의 임베딩 기법들이 주목받기 시작하였고, 최근 각광받는 **BERT<sup>Bidirectional Encoder Representations from Transformer</sup>**도 여기에 속함

### 규칙  설정 -> 엔드투엔드 -> 프리트레인/파인튜닝

- 90년대의 자연어 처리 모델은 대부분 사람이 피쳐를 직접 선정하고, 언어학적 지식을 갖춘 전문가가 **규칙<sup>rule</sup>**을 모델에 알려주는 방식이 주를 이루었음
- 2000년대 중반 이후 자연어 처리에서 딥러닝 모델이 주목받기 시작하였고, 데이터를 통째로 모델에 넣고 입출력 사이의 관계를 모델 스스로 학습하도록 유도하는 **엔드투엔드 모델<sup>end-to-end model</sup>**이 대표적이었음
- 2018년 **ELMo** 모델 이후에는 대규모 **말뭉치<sup>Corpus</sup>**로 임베딩을 만드는 **프리트레인<sup>pretrain</sup>**과정과, 임베딩을 입력으로 하는 딥러닝 모델 + 소규모 맞춤형 데이터로 모델 전체를 업데이트 하는 **파인튜닝<sup>fine tuning</sup>** 방식으로 발전하고 있음

## 임베딩 기법의 종류

### 행렬분해

- **행렬분해<sup>factorization/decomposition</sup>** 기법은 말뭉치 정보가 들어 있는 하나의 행렬을 두개 이상의 작은 행렬로 분해하는 임베딩 기법을 의미함

### 예측 기반

- 예측 기반 임베딩 기법은 어떤 단어 주변에 오거나 숨겨진 단어를 예측하는 방식으로 학습을 진행하며, 뉴럴 네트워크 모델을 주로 활용함

### 토픽 기반 방법

- 잠재된 주제를 추론하는 방식으로 **잠재 디리클레 할당<sup>LDA: Latent Dirichlet Allocation</sup>**이 가장 대표적인 기법임

## Definitions

- **말뭉치<sup>corpus</sup>**: 말뭉치는 수집된 텍스트 데이터 표본 전체를 의미
- **컬렉션<sup>collection</sup>**: 컬렉션은 말뭉치에 속한 집단을 의미하며, 예를 들어 분석을 위해 설문조사 데이터와 리뷰 데이터를 사용했다면 각각의 집합을 컬렉션이라고 지칭함
- **문장<sup>sentence</sup>**: 텍스트 데이터의 기본 단위로 완결된 의미를 나타내는 최소의 독립적인 형식 데이터를 의미하지만, 현실적으로는 마침표(.)나 느낌표(!), 물음표(?)와 같은 기호로 구분된 문자열로 보기도 함
- **문서<sup>document</sup>**: 문서나 단락은 동일한 생각이나 감정, 정보를 공유하는 문장 집합을 의미하며, 별도의 설명이 없다면 줄바꿈(\n) 문자로 구분되는 것이 일반적이기도 함
- **토큰<sup>token</sup>**: 단어<sup>word</sup>, 형태소<sup>morpheme</sup>, 서브워드<sup>subword</sup> 등으로 명칭되기도 하는 토큰은 문장을 구성하는 가장 작은 요소를 의미하며, 오픈소스 형태소 분석기에 따라 다른 방식으로 토큰을 분류하기도 함
- **토크나이즈<sup>tokenize</sup>**: 문장을 토큰 시퀀스로 분석하는 과정
- **어휘 집합<sup>vocabulary</sup>**: 문서를 문장으로 나누고, 토크나이즈를 실시한 후 중복을 제거한 토큰들의 집합
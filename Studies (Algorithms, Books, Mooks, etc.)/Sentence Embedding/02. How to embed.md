# 02. 벡터가 어떻게 의미를 갖게 되는가



## 임베딩에 자연어 의미를 어떻게 함축할 수 있을까?

- 임베딩은 자연어의 통계적 패턴 정보를 통째로 vector에 끼워 넣음

- 여기서 사용하는 통계적 패턴 정보는 크게 1) 어떤 단어가 (많이) 쓰였는지, 2) 단어가 어떤 순서로 등장하는지, 3)어떤 단어가 같이 사용되었는지, 이 3가지 방식이 보편적임:

  | 구분        | 백오브워즈                  | 언어 모델                   | 분포 가정                 |
  | ----------- | --------------------------- | --------------------------- | ------------------------- |
  | 내용        | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가 |
  | 대표 통계량 | `TF-IDF`                    | -                           | `PMI`                     |
  | 대표 모델   | `Deep Averaging Network`    | `ELMo`, `GPT`               | `Word2Vec`                |

- **백오브워즈<sup>bag of words</sup>** 가정에서는 어떤 단어가 많이 쓰였는지를 중시하며, 그 대척점에는 **언어모델<sup>language model</sup>**이 단어 시퀀스가 얼마나 자연스러운지 뉴럴 네트워크 기반의 모델로 확률을 부여함; **분포가정<sup>distribution hypothesis</sup>**에서는 주변 문맥(단어들)을 통해 통계적 패턴 정보를 유추함

- 위 3가지 방법은 말뭉치의 통계적 패턴을 이해하는 철학의 차이이며 상호 보완적이라고 볼 수 있음



## 어떤 단어가 많이 쓰였는가

### 백오브워즈 가정

- 백오브워즈는 중복을 허용한 단어의 집합으로, **단어의 등장 순서에 관계없이** 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 의미

- 경우에 따라서는 빈도 역시 `one-hot-encoding`으로 단순화 해 등장 여부 만을 백오브워즈 임베딩으로 쓰기도 함

- 가장 보편적인 백오브워즈는 **Term-Document matrix**로 아래와 같은 형태를 띔:

  | 구분   | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑방 손님과 어머니 | 삼포 가는 길 |
  | ------ | -------------- | ------------ | -------------------- | ------------ |
  | 기차   | 0              | 2            | 10                   | 7            |
  | 막걸리 | 0              | 1            | 0                    | 0            |
  | 선술집 | 0              | 1            | 0                    | 0            |

- 백오브워즈 임베딩 자체는 매우 간단한 아이디어이지만 **정보검색<sup>Information Retrieval</sup>** 분야에서 여전히 많이 쓰이고 있음 (문서 임베딩 간 코사인 유사도가 높은 문서)

### TF-IDF

- 백오브워즈 임베딩의 가장 큰 단점은, **'을/를', '이/가'**처럼 공통으로 매우 높은 등장 빈도를 나타내는 조사가 해당 문서의 주제를 추측하기 어렵게 만드는 점에 있음

- 이러한 단점을 보완하기 위해 제안된 기법이 Term-Document matrix에 아래와 같이 가중치를 계산해 행렬값을 바꾼 **TF-IDF<sup>Term Frequency-Inverse Document Frequency</sup>**임
  $$
  TF-IDF(w) = TF(w) \times log(\frac{N}{DF(w)})
  $$

- 하나씩 살펴보자:

  > - $TF(w)$는 백오브워즈 임베딩의 Term-document matrix의 행렬값을 의미
  > - $log\frac{N}{DF(w)}$는 위 행렬값에 적용될 가중치를 의미
  > - $DF(w)$는 document frequency로 말뭉치 전체에서 특정 단어$w$가 나타난 문서의 수를 의미함
  > - $N$은 전체 문서 수를 의미
  > - 결국 $IDF(w)$는 "전체 문서수"를 "$w$가 나타난 문서 수"로 나눈 후 로그를 취한 값으로 $N$과 $DF(w)$가 같을 경우에 $log(1)=0$으로 가중치를 부여

- 결국 **TF-IDF**의 경우 조사같이 정보성이 없는 단어들은 그 가중치가 0으로 확 줄게 돼 불필요한 정보가 사라지고 백오브워즈 보다는 좀 더 품질이 좋은 임베딩이라고 볼 수 있음

### Deep Averaging Network


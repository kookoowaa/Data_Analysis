# 02. 벡터가 어떻게 의미를 갖게 되는가



## 임베딩에 자연어 의미를 어떻게 함축할 수 있을까?

- 임베딩은 자연어의 통계적 패턴 정보를 통째로 vector에 끼워 넣음

- 여기서 사용하는 통계적 패턴 정보는 크게 1) 어떤 단어가 (많이) 쓰였는지, 2) 단어가 어떤 순서로 등장하는지, 3)어떤 단어가 같이 사용되었는지, 이 3가지 방식이 보편적임:

  | 구분        | 백오브워즈                  | 언어 모델                   | 분포 가정                 |
  | ----------- | --------------------------- | --------------------------- | ------------------------- |
  | 내용        | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가 |
  | 대표 통계량 | `TF-IDF`                    | -                           | `PMI`                     |
  | 대표 모델   | `Deep Averaging Network`    | `ELMo`, `GPT`               | `Word2Vec`                |

- **백오브워즈<sup>bag of words</sup>** 가정에서는 어떤 단어가 많이 쓰였는지를 중시하며, 그 대척점에는 **언어모델<sup>language model</sup>**이 단어 시퀀스가 얼마나 자연스러운지 뉴럴 네트워크 기반의 모델로 확률을 부여함; **분포가정<sup>distribution hypothesis</sup>**에서는 주변 문맥(단어들)을 통해 통계적 패턴 정보를 유추함

- 위 3가지 방법은 말뭉치의 통계적 패턴을 이해하는 철학의 차이이며 상호 보완적이라고 볼 수 있음



## 어떤 단어가 많이 쓰였는가

- 
# 02. 벡터가 어떻게 의미를 갖게 되는가



## 임베딩에 자연어 의미를 어떻게 함축할 수 있을까?

- 임베딩은 자연어의 통계적 패턴 정보를 통째로 vector에 끼워 넣음

- 여기서 사용하는 통계적 패턴 정보는 크게 1) 어떤 단어가 (많이) 쓰였는지, 2) 단어가 어떤 순서로 등장하는지, 3)어떤 단어가 같이 사용되었는지, 이 3가지 방식이 보편적임:

  | 구분        | 백오브워즈                  | 언어 모델                   | 분포 가정                 |
  | ----------- | --------------------------- | --------------------------- | ------------------------- |
  | 내용        | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가 |
  | 대표 통계량 | `TF-IDF`                    | -                           | `PMI`                     |
  | 대표 모델   | `Deep Averaging Network`    | `ELMo`, `GPT`               | `Word2Vec`                |

- **백오브워즈<sup>bag of words</sup>** 가정에서는 어떤 단어가 많이 쓰였는지를 중시하며, 그 대척점에는 **언어모델<sup>language model</sup>**이 단어 시퀀스가 얼마나 자연스러운지 뉴럴 네트워크 기반의 모델로 확률을 부여함; **분포가정<sup>distribution hypothesis</sup>**에서는 주변 문맥(단어들)을 통해 통계적 패턴 정보를 유추함

- 위 3가지 방법은 말뭉치의 통계적 패턴을 이해하는 철학의 차이이며 상호 보완적이라고 볼 수 있음



## 어떤 단어가 많이 쓰였는가

### 백오브워즈 가정

- 백오브워즈는 중복을 허용한 단어의 집합으로, **단어의 등장 순서에 관계없이** 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 의미

- 경우에 따라서는 빈도 역시 `one-hot-encoding`으로 단순화 해 등장 여부 만을 백오브워즈 임베딩으로 쓰기도 함

- 가장 보편적인 백오브워즈는 **Term-Document matrix**로 아래와 같은 형태를 띔:

  | 구분   | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑방 손님과 어머니 | 삼포 가는 길 |
  | ------ | -------------- | ------------ | -------------------- | ------------ |
  | 기차   | 0              | 2            | 10                   | 7            |
  | 막걸리 | 0              | 1            | 0                    | 0            |
  | 선술집 | 0              | 1            | 0                    | 0            |

- 백오브워즈 임베딩 자체는 매우 간단한 아이디어이지만 **정보검색<sup>Information Retrieval</sup>** 분야에서 여전히 많이 쓰이고 있음 (문서 임베딩 간 코사인 유사도가 높은 문서)

### TF-IDF

- 백오브워즈 임베딩의 가장 큰 단점은, **'을/를', '이/가'**처럼 공통으로 매우 높은 등장 빈도를 나타내는 조사가 해당 문서의 주제를 추측하기 어렵게 만드는 점에 있음

- 이러한 단점을 보완하기 위해 제안된 기법이 Term-Document matrix에 아래와 같이 가중치를 계산해 행렬값을 바꾼 **TF-IDF<sup>Term Frequency-Inverse Document Frequency</sup>**임
  $$
  TF-IDF(w) = TF(w) \times log(\frac{N}{DF(w)})
  $$

- 하나씩 살펴보자:

  > - $TF(w)$는 백오브워즈 임베딩의 Term-document matrix의 행렬값을 의미
  > - $log\frac{N}{DF(w)}$는 위 행렬값에 적용될 가중치를 의미
  > - $DF(w)$는 document frequency로 말뭉치 전체에서 특정 단어$w$가 나타난 문서의 수를 의미함
  > - $N$은 전체 문서 수를 의미
  > - 결국 $IDF(w)$는 "전체 문서수"를 "$w$가 나타난 문서 수"로 나눈 후 로그를 취한 값으로 $N$과 $DF(w)$가 같을 경우에 $log(1)=0$으로 가중치를 부여

- 결국 **TF-IDF**의 경우 조사같이 정보성이 없는 단어들은 그 가중치가 0으로 확 줄게 돼 불필요한 정보가 사라지고 백오브워즈 보다는 좀 더 품질이 좋은 임베딩이라고 볼 수 있음

### Deep Averaging Network

- Deep Averaging Network(*Iyyer et al., 2015*)는 뉴럴 네트워크 버전의 백오브워즈 가정임:


$$
V = \frac{1}{N}\sum^N_{i=1}e_i
$$

- 백오브워즈와 마찬가지로 단어의 순서를 고려하지 않으며, 위의 수식과 같이 집합에 속한 단어의 임베딩을 평균을 취하는 방식으로 만듦 (벡터의 덧셈은 교환법칙이 성립하므로 순서와 무관)
- Iyyer et al. (2015)는 간단한 구조의 아키텍처임에도 불구하고, 위와 같은($V$) 문장 임베딩을 입력으로하는 분류기에서  좋은 성능을 발휘해 왔음



## 단어가 어떤 순서로 쓰였는가

> - **언어 모델<sup>language model</sup>**은 단어 시퀀스를 명시적으로 학습, 확률을 부여하는 모델로 백오브워즈의 대척점에 있다고도 볼 수 있음 (단어가 $n$개일 때 $P(w_1, w_2, w_3, ... , w_n)$을 반환)

### 통계 기반 언어 모델

- **통계 기반** 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습함

- 예를 들어 "두시 삼십이분"이라는 시퀀스가 0.51이라는 확률값을 반환할 때 "이시 서른두분"은 0.08이라는 확률값을 반환하며 자연스러운 한국어 문장에 높은 확률 값을 부여함

- 통계 기반 언어 모델을 `내 마음 속에 영원히 기억될 최고의 명작이다`라는 예시로 살펴보면 다음과 같음:

  > - 네이버 리뷰에서 `내 마음 속에 영원히 기억될 최고의 명작이다`의 등장빈도를 살펴보면 0회로, 통계 기반 언어 모델이 해당 표현을 나타낼 확률은 0으로 부여하게 됨
  >
  >   | 표현                                       | 빈도 |
  >   | ------------------------------------------ | ---- |
  >   | 내                                         | 1309 |
  >   | 마음                                       | 172  |
  >   | 속에                                       | 155  |
  >   | ...                                        | ...  |
  >   | 최고의 명작이다                            | 23   |
  >   | 영원히 기억될 최고의 명작이다              | 1    |
  >   | 내 마음 속에 영원히 기억될 최고의 명작이다 | 0    |
  >
  > - `내 마음 속에 영원히 기억될 최고의` 다음에  `명작이다`라는 단어가 나타날 확률을 조건부확률의 정의를 활용해 **최대우도추정법<sup>Maximum Likelihood Estimation</sup>**으로 유도하면 다음 수식과 같음: 
  >
  >   $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) = \frac{Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)}{Freq(내, 마음, 속에, 영원히, 기억될, 최고의)}$$
  >
  > - 이를 $n$개 단어씩 묶어서 학습하는 **n-gram** 모델을 써서 일부 해결할 수 있음
  >
  > - **마르코프 가정<sup>Markov assumption</sup>**에 기반하여 직전 $n-1$개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사<sup>approximation</sup>하는 것으로 가능
  >
  > -  `내 마음 속에 영원히 기억될 최고의` 다음에 `명작이다`가 나타날 확률을 bi-gram (2-gram) 모델로 근사하면 다음 수식과 같음: 
  >
  >   $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) \approx P(명작이다|최고의) = \frac{Freq(최고의, 명작이다)}{Freq(최고의)}$$
  >
  > - 마찬가지로 bi-gram 모델로 `내 마음 속에 영원히 기억될 최고의 명작이다` 시퀀스가 나타날 확률도 계산할 수 있으며, 이 때 각 bi-gram의 등장확률을 슬라이딩 해가면서 끝까지 곱한 결과가 다음 수식이 됨: 
  >
  >   $$P(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)
  >    \approx P(내) \times P(마음|내) \times P(속에|마음) \times P(영원히|속에) \times P(기억될|영원히) \times P(최고의|기억될) \times P(명작이다|최고의)$$
  >
  > - 위의 bi-gram 모델을 일반화 하면 다음과 같이 표현할 수 있음
  >
  >   $$P(w_n|w_{n-1}) = \frac{Freq(w_{n-1}, w_n)}{Freq(w_{n-1})}$$ 
  >
  >   $$P(w^n_1) = P(w_1, w_2, ... , w_n) = \prod^n_{k=1}P(w_k|w_{k-1})$$
  >
  > - 그럼에도 불구하고, n-gram모델은 여전히 등장 빈도가 0인 케이스에 취약할 수 밖에 없음
  >
  > - 이를 보완하기 위해 **백오프<sup>back-off</sup>**, **스무딩<sup>smoothing</sup>** 등의 방식이 고안되었음
  >
  > - 백오프는 특정 n-gram의 빈도가 0일경우 $n-k$만큼 시퀀스를 축소하고 대신 $\alpha, \beta$ 파라미터로 실제 빈도와의 차이를 보정하며 학습하는 방식으로 아래 예시와(7-gram 모델을 축소하여 4-gram 모델로 백오프) 같이 활용됨:
  >
  >   $$Freq( 내 마음 속에 영원히 기억될 최고의 명작이다) \approx \alpha Freq(영원히 기억될 최고의 명작이다) + \beta$$
  >
  > - 스무딩은 등장 빈도 표에 모두 $k$만큼 더하는 기법으로 높은 빈도를 가진 문자열 등장 확률을 일부 깎고, 빈도가 0인 케이스에 작지만 일부 확률을 부여하게 됨

  ###  뉴럴 네트워크 기반 언어 모델

  
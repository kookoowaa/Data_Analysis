# 02. 벡터가 어떻게 의미를 갖게 되는가



## 임베딩에 자연어 의미를 어떻게 함축할 수 있을까?

- 임베딩은 자연어의 통계적 패턴 정보를 통째로 vector에 끼워 넣음

- 여기서 사용하는 통계적 패턴 정보는 크게 1) 어떤 단어가 (많이) 쓰였는지, 2) 단어가 어떤 순서로 등장하는지, 3)어떤 단어가 같이 사용되었는지, 이 3가지 방식이 보편적임:

  | 구분        | 백오브워즈                  | 언어 모델                   | 분포 가정                 |
  | ----------- | --------------------------- | --------------------------- | ------------------------- |
  | 내용        | 어떤 단어가 (많이) 쓰였는가 | 단어가 어떤 순서로 쓰였는가 | 어떤 단어가 같이 쓰였는가 |
  | 대표 통계량 | `TF-IDF`                    | -                           | `PMI`                     |
  | 대표 모델   | `Deep Averaging Network`    | `ELMo`, `GPT`               | `Word2Vec`                |

- **백오브워즈<sup>bag of words</sup>** 가정에서는 어떤 단어가 많이 쓰였는지를 중시하며, 그 대척점에는 **언어모델<sup>language model</sup>**이 단어 시퀀스가 얼마나 자연스러운지 뉴럴 네트워크 기반의 모델로 확률을 부여함; **분포가정<sup>distribution hypothesis</sup>**에서는 주변 문맥(단어들)을 통해 통계적 패턴 정보를 유추함

- 위 3가지 방법은 말뭉치의 통계적 패턴을 이해하는 철학의 차이이며 상호 보완적이라고 볼 수 있음



## 어떤 단어가 많이 쓰였는가

### 백오브워즈 가정

- 백오브워즈는 중복을 허용한 단어의 집합으로, **단어의 등장 순서에 관계없이** 문서 내 단어의 등장 빈도를 임베딩으로 쓰는 기법을 의미

- 경우에 따라서는 빈도 역시 `one-hot-encoding`으로 단순화 해 등장 여부 만을 백오브워즈 임베딩으로 쓰기도 함

- 가장 보편적인 백오브워즈는 **Term-Document matrix**로 아래와 같은 형태를 띔:

  | 구분   | 메밀꽃 필 무렵 | 운수 좋은 날 | 사랑방 손님과 어머니 | 삼포 가는 길 |
  | ------ | -------------- | ------------ | -------------------- | ------------ |
  | 기차   | 0              | 2            | 10                   | 7            |
  | 막걸리 | 0              | 1            | 0                    | 0            |
  | 선술집 | 0              | 1            | 0                    | 0            |

- 백오브워즈 임베딩 자체는 매우 간단한 아이디어이지만 **정보검색<sup>Information Retrieval</sup>** 분야에서 여전히 많이 쓰이고 있음 (문서 임베딩 간 코사인 유사도가 높은 문서)

### TF-IDF

- 백오브워즈 임베딩의 가장 큰 단점은, **'을/를', '이/가'**처럼 공통으로 매우 높은 등장 빈도를 나타내는 조사가 해당 문서의 주제를 추측하기 어렵게 만드는 점에 있음

- 이러한 단점을 보완하기 위해 제안된 기법이 Term-Document matrix에 아래와 같이 가중치를 계산해 행렬값을 바꾼 **TF-IDF<sup>Term Frequency-Inverse Document Frequency</sup>**임
  $$
  TF-IDF(w) = TF(w) \times log(\frac{N}{DF(w)})
  $$

- 하나씩 살펴보자:

  > - $TF(w)$는 백오브워즈 임베딩의 Term-document matrix의 행렬값을 의미
  > - $log\frac{N}{DF(w)}$는 위 행렬값에 적용될 가중치를 의미
  > - $DF(w)$는 document frequency로 말뭉치 전체에서 특정 단어$w$가 나타난 문서의 수를 의미함
  > - $N$은 전체 문서 수를 의미
  > - 결국 $IDF(w)$는 "전체 문서수"를 "$w$가 나타난 문서 수"로 나눈 후 로그를 취한 값으로 $N$과 $DF(w)$가 같을 경우에 $log(1)=0$으로 가중치를 부여

- 결국 **TF-IDF**의 경우 조사같이 정보성이 없는 단어들은 그 가중치가 0으로 확 줄게 돼 불필요한 정보가 사라지고 백오브워즈 보다는 좀 더 품질이 좋은 임베딩이라고 볼 수 있음

### Deep Averaging Network

- Deep Averaging Network(*Iyyer et al., 2015*)는 뉴럴 네트워크 버전의 백오브워즈 가정임:


$$
V = \frac{1}{N}\sum^N_{i=1}e_i
$$

- ![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2F3GgtS%2FbtqBuQ2aPIU%2F3FBFDIpbL7FedUj46YoGUk%2Fimg.png)
- 백오브워즈와 마찬가지로 단어의 순서를 고려하지 않으며, 위의 수식과 같이 집합에 속한 단어의 임베딩을 평균을 취하는 방식으로 학습하며, softmax를 통해 분류하는 모델임 (벡터의 덧셈은 교환법칙이 성립하므로 순서와 무관)
- Iyyer et al. (2015)는 간단한 구조의 아키텍처임에도 불구하고, 위와 같은($V$) 문장 임베딩을 입력으로하는 분류기에서  좋은 성능을 발휘해 왔음



## 단어가 어떤 순서로 쓰였는가

> - **언어 모델<sup>language model</sup>**은 단어 시퀀스를 명시적으로 학습, 확률을 부여하는 모델로 백오브워즈의 대척점에 있다고도 볼 수 있음 (단어가 $n$개일 때 $P(w_1, w_2, w_3, ... , w_n)$을 반환)

### 통계 기반 언어 모델

- **통계 기반** 언어 모델은 말뭉치에서 해당 단어 시퀀스가 얼마나 자주 등장하는지 빈도를 세어 학습함

- 예를 들어 "두시 삼십이분"이라는 시퀀스가 0.51이라는 확률값을 반환할 때 "이시 서른두분"은 0.08이라는 확률값을 반환하며 자연스러운 한국어 문장에 높은 확률 값을 부여함

- 통계 기반 언어 모델을 `내 마음 속에 영원히 기억될 최고의 명작이다`라는 예시로 살펴보면 다음과 같음:

  > - 네이버 리뷰에서 `내 마음 속에 영원히 기억될 최고의 명작이다`의 등장빈도를 살펴보면 0회로, 통계 기반 언어 모델이 해당 표현을 나타낼 확률은 0으로 부여하게 됨
  >
  >   | 표현                                       | 빈도 |
  >   | ------------------------------------------ | ---- |
  >   | 내                                         | 1309 |
  >   | 마음                                       | 172  |
  >   | 속에                                       | 155  |
  >   | ...                                        | ...  |
  >   | 최고의 명작이다                            | 23   |
  >   | 영원히 기억될 최고의 명작이다              | 1    |
  >   | 내 마음 속에 영원히 기억될 최고의 명작이다 | 0    |
  >
  > - `내 마음 속에 영원히 기억될 최고의` 다음에  `명작이다`라는 단어가 나타날 확률을 조건부확률의 정의를 활용해 **최대우도추정법<sup>Maximum Likelihood Estimation</sup>**으로 유도하면 다음 수식과 같음: 
  >
  >   $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) = \frac{Freq(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)}{Freq(내, 마음, 속에, 영원히, 기억될, 최고의)}$$
  >
  > - 이를 $n$개 단어씩 묶어서 학습하는 **n-gram** 모델을 써서 일부 해결할 수 있음
  >
  > - **마르코프 가정<sup>Markov assumption</sup>**에 기반하여 직전 $n-1$개 단어의 등장 확률로 전체 단어 시퀀스 등장 확률을 근사<sup>approximation</sup>하는 것으로 가능
  >
  > -  `내 마음 속에 영원히 기억될 최고의` 다음에 `명작이다`가 나타날 확률을 bi-gram (2-gram) 모델로 근사하면 다음 수식과 같음: 
  >
  >   $$P(명작이다|내, 마음, 속에, 영원히, 기억될, 최고의) \approx P(명작이다|최고의) = \frac{Freq(최고의, 명작이다)}{Freq(최고의)}$$
  >
  > - 마찬가지로 bi-gram 모델로 `내 마음 속에 영원히 기억될 최고의 명작이다` 시퀀스가 나타날 확률도 계산할 수 있으며, 이 때 각 bi-gram의 등장확률을 슬라이딩 해가면서 끝까지 곱한 결과가 다음 수식이 됨: 
  >
  >   $$P(내, 마음, 속에, 영원히, 기억될, 최고의, 명작이다)
  >    \approx P(내) \times P(마음|내) \times P(속에|마음) \times P(영원히|속에) \times P(기억될|영원히) \times P(최고의|기억될) \times P(명작이다|최고의)$$
  >
  > - 위의 bi-gram 모델을 일반화 하면 다음과 같이 표현할 수 있음
  >
  >   $$P(w_n|w_{n-1}) = \frac{Freq(w_{n-1}, w_n)}{Freq(w_{n-1})}$$ 
  >
  >   $$P(w^n_1) = P(w_1, w_2, ... , w_n) = \prod^n_{k=1}P(w_k|w_{k-1})$$
  >
  > - 그럼에도 불구하고, n-gram모델은 여전히 등장 빈도가 0인 케이스에 취약할 수 밖에 없음
  >
  > - 이를 보완하기 위해 **백오프<sup>back-off</sup>**, **스무딩<sup>smoothing</sup>** 등의 방식이 고안되었음
  >
  > - 백오프는 특정 n-gram의 빈도가 0일경우 $n-k$만큼 시퀀스를 축소하고 대신 $\alpha, \beta$ 파라미터로 실제 빈도와의 차이를 보정하며 학습하는 방식으로 아래 예시와(7-gram 모델을 축소하여 4-gram 모델로 백오프) 같이 활용됨:
  >
  >   $$Freq( 내 마음 속에 영원히 기억될 최고의 명작이다) \approx \alpha Freq(영원히 기억될 최고의 명작이다) + \beta$$
  >
  > - 스무딩은 등장 빈도 표에 모두 $k$만큼 더하는 기법으로 높은 빈도를 가진 문자열 등장 확률을 일부 깎고, 빈도가 0인 케이스에 작지만 일부 확률을 부여하게 됨

  ###  뉴럴 네트워크 기반 언어 모델

  - 위의 통계 기반 언어 모델이 단어들의 빈도를 세어서 학습하는 반면, 뉴럴 네트워크 기반 언어 모델은 **주어진 단어 시퀀스를 가지고 다음 단어를 맞추는 과정에서 학습**을 진행 (ELMo, GPT 등)
  
    ```
    발 없는 말이 > {언어 모델} > "천리"
    ```
  
  - 혹은 이와 유사하지만, 문장 중간에 마스크를 씌워 놓고 해당 마스크 위치에 어떤 단어가 올지 예측하는 **마스크 언어 모델<sup>masked language model</sup>**도 있음
  
  - 마스크 언어 모델의 경우 순차적으로 단어를 입력받아 다음 단어를 예측하는 일방향<sup>uni-directional</sup>이 아닌 양방향<sup>bi-directional</sup> 학습을 통해 중간에 위치한 단어를 예측하기 때문에 기존 언어 모델 기법 대비 임베딩 품질이 좋은 편이며 BERT가 대표적인 마스크 언어 모델이라 할 수 있음
  
    ```
    발 없는 말이 [MASK] 간다 > {언어 모델} > "천리"
    ```



## 어떤 단어가 같이 쓰였는가

### 분포 가정

- 분포 가정은 언어학자 비트겐슈타인(1889~1951)의 "단어의 의미는 곧 그 언어에서의 활용이다" 라는 철학에 기반해 있음
- 이는 어떤 단어 그룹이 비슷한 문맥 환경에서 자주 등장한다면 그 의미 또한 유사할 것이라는 **분포 가정<sup>distributional hypothesis</sup>**를 전제로 함
- 예를 들어 `빨래`, `세탁` 이라는 타깃 단어가 사용 된 위키 백과 문맥을 발췌하면, `청소`, `물` 등은 반복적으로 함께 등장하는 문맥 단어가 되며, 타깃 단어는 비슷한 의미를 지닐 가능성이 높다고 결론 내릴 수 있음
- 얼핏 백오브워즈를 비롯한 빈도 기반의 임베딩과 유사해 보일 수 있으나, 빈도 기반의 임베딩이 [0 0 0 0 0 1 ... 0] 같이 고차원에 각 차원을 분리하여 표현하지만, 분포 가정은 [0.2 0.3 0.5 0.7 ... 0.2] 와 같이 저차원에 의미를 분산하여 표현

### 분포와 의미 (1): 형태소

- 언어학에서 **형태소<sup>morpheme</sup>**이란 의미를 갖는 최소 단위를 의미함: (`강아지`란 형태소를 `강`과 `아지`로 나누는 순간 `강아지`의 의미가 사라짐)
- 언어학 관점에서 형태소를 분석하는 대표적인 방법으로는 **계열 관계<sup>paradigmatic relation</sup>**이 있으며, 계열 관계는 해당 형태소 자리에 다른 형태소가 **"대치"**되어 사용 될 수 있는지, 다시 이야기 하면 주변의 문맥 정보를 바탕으로 형태소를 확인한다고 볼 수 있음
- 즉, 분포 가정은 형태소와 밀접한 관계를 이루고 있다고 볼 수 있음

### 분포와 의미 (2): 품사

- 다른 관점에서 보면, 단어를 **기능<sup>function</sup>**, **의미<sup>meaning</sup>**, **형식<sup>form</sup>** 세가지 기준에 따라 품사를 분류하기도 함
- 예를 들어 `깊이`와 `깊다`는 같은 의미를 갖지만 기능이 다르고, `깊이`와 `높이`는 의미는 다르지만 주어나 목적어처럼 같은 기능을 수행함 
- 단, 언어학자들이 실제 품사를 분류 할 때 결정적인 기준은 '기능'이라고 하며, '기능'은 실제로 단어의 분포와 매우 밀접한 관련을 맺고 있음
- 요컨대, 형태소나 품사를 분류함에 있어 다양한 언어학적 문제는 말뭉치의 분포 정보와 깊은 관계를 갖고 있다고 이해할 수 있으며, 임베딩에 분포 정보를 함축함으로써 벡터 내 의미를 자연스럽게 내재시킬 수 있음

### 점별 상호 정보량<sup>PMI</sup>

- **점별 상호 정보량<sup>PMI, Pointwise Mutual Information</sup>**은 두 확률변수 사이의 상관성을 계량화 하는 단위이며, 두 확률변수가 완전히 독립일 경우에는 그 값이 0이 됨

- PMI는 두 단어가 얼마나 자주 같이 등장하는지에 관한 정보를 수치화 한 것으로 다음과 같은 공식은 따름:
  $$PMI(A, B) = log\frac{P(A, B)}{P(A)P(B)}$$

- 예를 들어 $PMI(고기, 밥)$수식을 적용하는 과정은 다음과 같음:

  > - **단어-문맥 행렬<sup>word-context matrix</sup>** 구성으로 부터 시작
  >
  >   | 구분  | ...  | 고기 | 와   | 밥   | 을   | 먹는다 | ...  | total |
  >   | ----- | ---- | ---- | ---- | ---- | ---- | ------ | ---- | ----- |
  >   | ...   |      |      |      |      |      |        |      |       |
  >   | 밥    |      | +1   | +1   |      | +1   | +1     |      | 20    |
  >   | ...   |      |      |      |      |      |        |      |       |
  >   | total |      | 15   |      | 20   |      |        |      | 1000  |
  >
  > - 고기의 등장 빈도가 15, 밥의 등장 빈도가 20, 둘의 동시 등장 빈도가 10이라고 가정하였을 때 PMI를 계산하면 아래와 같음:
  >
  >   $$PMI(고기, 밥) = log\frac{P(고기, 밥)}{P(고기)P(밥)} = log\frac{\frac{10}{1000}}{\frac{15}{1000}\frac{20}{1000}}$$

### Word2Vec

- 분포 가정의 가장 대표적인 모델은 2013년 구글 연구팀이 발표한 Word2Vec이라는 임베딩 기법임
- **CBOW<sup>Continuous Bag of Words</sup>** 방식은 주변의 문맥 단어로 타깃 단어를 맞추는 과정을 통해 학습을 진행하고, 반대로 **Skip-Gram**은 타깃 단어로 주변의 문맥 단어를 맞추며 학습을 진행
- 여기서 주변의 문맥 단어는 윈도우 크기에 따라 결정됨: (윈도우를 타깃 단어로)
  여기서 주변의 문맥 **단어는** *윈도우* **크기에** 따라 결정됨 (윈도우 1)
  여기서 **주변의 문맥 단어는** *윈도우* **크기에 따라 결정됨** (윈도우 3)
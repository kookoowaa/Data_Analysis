{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Softmax-with-Loss 계산 그래프\n",
    "![](image/fig a-1.png)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 순전파\n",
    "### > Softmax 계층의 함수 수식\n",
    "$$\n",
    "y_k = \\frac{exp(a_k)}{\\sum_{i=1}^n exp(a_i)}\n",
    "$$\n",
    "<br>\n",
    "\n",
    "### > Softmax계층의 계산 그래프\n",
    "![](image/fig a-2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Cross Entropy 계층의 함수 수식\n",
    "$$\n",
    "L = -\\sum_k t_k logy_k\n",
    "$$\n",
    "<br>\n",
    "\n",
    "### > Cross Entropy 계층의 계산 그래프\n",
    "![](image/fig a-3.png)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A2. 역전파\n",
    "### > Cross Entropy 계층의 역전파\n",
    "![](image/fig a-4.png)\n",
    "\n",
    "> - 역전파의 초기값은 $\\frac{\\delta L}{\\delta L} = 1$임\n",
    "> - 'x' 노드의 역전파는 순전파 시의 입력값을 서로 바꿈\n",
    ">- '+' 노드의 역전파는 순전파 시의 입력값을 그대로 전달\n",
    ">- 'log' 노드의 역전파는 $\\frac{\\delta y}{\\delta x} = \\frac{1}{x}$ 값을 전달"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### > Softmax 계층의 역전파\n",
    "생략 (6단계)\n",
    "\n",
    "### > 정리\n",
    "![](image/fig a-5.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
